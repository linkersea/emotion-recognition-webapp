"""
æƒ…ç»ªè¯†åˆ«æ¨ç†æ¨¡å—
"""

import os
import yaml
import torch
import torch.nn.functional as F
import numpy as np
from PIL import Image
import cv2
from torchvision import transforms

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from models.architectures.resnet_model import create_model


class EmotionPredictor:
    """æƒ…ç»ªè¯†åˆ«é¢„æµ‹å™¨"""
    
    def __init__(self, model_path, config_path):
        """
        Args:
            model_path: æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        # åŠ è½½é…ç½®
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        # è®¾å¤‡è®¾ç½®
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # æ ¹æ®æ¨¡å‹æ–‡ä»¶åç¡®å®šæ¶æ„ç±»å‹
        model_filename = os.path.basename(model_path)
        print(f"ğŸ” æ£€æµ‹æ¨¡å‹ç±»å‹: {model_filename}")
        
        if 'VGG' in model_filename or 'vgg' in model_filename:
            print(f"ğŸ“‹ åŠ è½½VGGæ¶æ„")
            from models.architectures.vgg_model import EmotionVGG
            self.model = EmotionVGG(num_classes=self.config['model']['num_classes'])
            self.is_rgb_model = True  # VGGéœ€è¦RGBè¾“å…¥
        elif 'CustomCNN' in model_filename or 'custom' in model_filename:
            print(f"ğŸ“‹ åŠ è½½Custom CNNæ¶æ„")
            from models.architectures.vgg_model import CustomEmotionCNN
            self.model = CustomEmotionCNN(num_classes=self.config['model']['num_classes'])
            self.is_rgb_model = False  # Custom CNNä½¿ç”¨ç°åº¦è¾“å…¥
        else:
            print(f"ğŸ“‹ åŠ è½½ResNetæ¶æ„")
            from models.architectures.resnet_model import create_model
            self.model = create_model(self.config)
            self.is_rgb_model = False  # ResNetä½¿ç”¨ç°åº¦è¾“å…¥
        
        # åŠ è½½è®­ç»ƒå¥½çš„æƒé‡
        checkpoint = torch.load(model_path, map_location=self.device)
        
        # æ£€æŸ¥æ£€æŸ¥ç‚¹æ ¼å¼
        if 'model_state_dict' in checkpoint:
            model_state = checkpoint['model_state_dict']
        else:
            model_state = checkpoint
        
        try:
            self.model.load_state_dict(model_state)
            print(f"âœ… æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ æƒé‡åŠ è½½è­¦å‘Š: {e}")
            # å°è¯•å…¼å®¹æ€§åŠ è½½
            model_dict = self.model.state_dict()
            filtered_state = {k: v for k, v in model_state.items() if k in model_dict and model_dict[k].shape == v.shape}
            model_dict.update(filtered_state)
            self.model.load_state_dict(model_dict)
            print(f"âœ… å…¼å®¹æ€§åŠ è½½å®Œæˆ")
        
        self.model.to(self.device)
        self.model.eval()
        
        # æƒ…ç»ªæ ‡ç­¾æ˜ å°„
        self.emotion_labels = self.config['emotion_labels']
        self.emotion_emojis = self.config['emotion_emojis']
        
        # æ ¹æ®æ¨¡å‹ç±»å‹è®¾ç½®æ•°æ®é¢„å¤„ç†
        if self.is_rgb_model:
            # VGGæ¨¡å‹ä½¿ç”¨RGBé¢„å¤„ç†
            self.transform = transforms.Compose([
                transforms.Resize((48, 48)),
                transforms.Lambda(lambda x: x.convert('RGB') if x.mode != 'RGB' else x),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
            print(f"ğŸ“Š ä½¿ç”¨RGBé¢„å¤„ç†")
        else:
            # ResNetå’ŒCustom CNNä½¿ç”¨ç°åº¦é¢„å¤„ç†
            self.transform = transforms.Compose([
                transforms.Grayscale(num_output_channels=1),
                transforms.Resize((48, 48)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485], std=[0.229])
            ])
            print(f"ğŸ“Š ä½¿ç”¨ç°åº¦é¢„å¤„ç†")
        
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def preprocess_image(self, image):
        """é¢„å¤„ç†å›¾åƒ"""
        if isinstance(image, str):
            # ä»æ–‡ä»¶è·¯å¾„åŠ è½½å›¾åƒ
            image = Image.open(image).convert('RGB')
        elif isinstance(image, np.ndarray):
            # ä»numpyæ•°ç»„è½¬æ¢
            if len(image.shape) == 3 and image.shape[2] == 3:
                # BGR to RGB
                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            image = Image.fromarray(image)
        elif not isinstance(image, Image.Image):
            raise ValueError("ä¸æ”¯æŒçš„å›¾åƒæ ¼å¼")
        
        # åº”ç”¨å˜æ¢
        image_tensor = self.transform(image).unsqueeze(0)
        return image_tensor.to(self.device)
    
    def predict(self, image):
        """é¢„æµ‹å•å¼ å›¾åƒçš„æƒ…ç»ª"""
        try:
            with torch.no_grad():
                # é¢„å¤„ç†å›¾åƒ
                image_tensor = self.preprocess_image(image)
                
                # æ¨¡å‹æ¨ç†
                outputs = self.model(image_tensor)
                probabilities = F.softmax(outputs, dim=1)
                
                # è·å–é¢„æµ‹ç»“æœ
                predicted_class = torch.argmax(probabilities, dim=1).item()
                confidence = probabilities[0][predicted_class].item()
                
                # è·å–æ‰€æœ‰ç±»åˆ«çš„æ¦‚ç‡
                all_probabilities = probabilities[0].cpu().numpy()
                
                # å®‰å…¨åœ°è·å–æƒ…ç»ªæ ‡ç­¾
                if str(predicted_class) in self.emotion_labels:
                    predicted_emotion = self.emotion_labels[str(predicted_class)]
                else:
                    # å¤‡ç”¨æ˜ å°„
                    emotion_names = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']
                    predicted_emotion = emotion_names[predicted_class] if predicted_class < len(emotion_names) else 'unknown'
                
                # å®‰å…¨åœ°è·å–emoji
                emoji = self.emotion_emojis.get(predicted_emotion, 'ğŸ˜')
                
                # æ„å»ºç»“æœ
                result = {
                    'predicted_emotion': predicted_emotion,
                    'predicted_class_id': predicted_class,
                    'confidence': confidence,
                    'emoji': emoji,
                    'all_probabilities': {}
                }
                
                # å®‰å…¨åœ°æ„å»ºæ‰€æœ‰æ¦‚ç‡
                emotion_names = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']
                for i, prob in enumerate(all_probabilities):
                    if i < len(emotion_names):
                        result['all_probabilities'][emotion_names[i]] = float(prob)
                
                return result
                
        except Exception as e:
            print(f"âŒ é¢„æµ‹è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {e}")
            # è¿”å›é»˜è®¤ç»“æœ
            return {
                'predicted_emotion': 'neutral',
                'predicted_class_id': 4,
                'confidence': 0.0,
                'emoji': 'ğŸ˜',
                'all_probabilities': {
                    'angry': 0.0, 'disgust': 0.0, 'fear': 0.0, 'happy': 0.0,
                    'neutral': 1.0, 'sad': 0.0, 'surprise': 0.0
                },
                'error': str(e)
            }
            
            return result
    
    def predict_batch(self, images):
        """æ‰¹é‡é¢„æµ‹å›¾åƒæƒ…ç»ª"""
        results = []
        for image in images:
            result = self.predict(image)
            results.append(result)
        return results
    
    def get_top_k_predictions(self, image, k=3):
        """è·å–top-ké¢„æµ‹ç»“æœ"""
        with torch.no_grad():
            image_tensor = self.preprocess_image(image)
            outputs = self.model(image_tensor)
            probabilities = F.softmax(outputs, dim=1)
            
            # è·å–top-kç»“æœ
            top_k_probs, top_k_indices = torch.topk(probabilities, k)
            
            results = []
            for i in range(k):
                class_id = top_k_indices[0][i].item()
                prob = top_k_probs[0][i].item()
                emotion = self.emotion_labels[str(class_id)]
                
                results.append({
                    'emotion': emotion,
                    'probability': prob,
                    'emoji': self.emotion_emojis[emotion]
                })
            
            return results


class RealTimeEmotionDetector:
    """å®æ—¶æƒ…ç»ªæ£€æµ‹å™¨ï¼ˆç”¨äºæ‘„åƒå¤´è¾“å…¥ï¼‰"""
    
    def __init__(self, model_path, config_path):
        self.predictor = EmotionPredictor(model_path, config_path)
        
        # äººè„¸æ£€æµ‹å™¨
        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
    
    def detect_and_predict(self, frame):
        """æ£€æµ‹äººè„¸å¹¶é¢„æµ‹æƒ…ç»ª"""
        # è½¬æ¢ä¸ºç°åº¦å›¾è¿›è¡Œäººè„¸æ£€æµ‹
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # æ£€æµ‹äººè„¸
        faces = self.face_cascade.detectMultiScale(
            gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30)
        )
        
        results = []
        
        for (x, y, w, h) in faces:
            # æå–äººè„¸åŒºåŸŸ
            face_roi = frame[y:y+h, x:x+w]
            
            # é¢„æµ‹æƒ…ç»ª
            emotion_result = self.predictor.predict(face_roi)
            
            # æ·»åŠ ä½ç½®ä¿¡æ¯
            emotion_result['face_bbox'] = (x, y, w, h)
            results.append(emotion_result)
        
        return results
    
    def draw_emotion_on_frame(self, frame, results):
        """åœ¨å¸§ä¸Šç»˜åˆ¶æƒ…ç»ªä¿¡æ¯"""
        for result in results:
            x, y, w, h = result['face_bbox']
            emotion = result['predicted_emotion']
            confidence = result['confidence']
            emoji = result['emoji']
            
            # ç»˜åˆ¶äººè„¸æ¡†
            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
            
            # ç»˜åˆ¶æƒ…ç»ªä¿¡æ¯
            label = f"{emotion} {emoji} ({confidence:.2f})"
            label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)[0]
            
            # èƒŒæ™¯çŸ©å½¢
            cv2.rectangle(frame, (x, y-30), (x+label_size[0], y), (0, 255, 0), -1)
            
            # æ–‡å­—
            cv2.putText(frame, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)
        
        return frame


def demo_single_image():
    """å•å¼ å›¾åƒé¢„æµ‹æ¼”ç¤º"""
    # é…ç½®æ–‡ä»¶è·¯å¾„
    config_path = 'configs/config.yaml'
    model_path = 'models/saved_models/emotion_resnet_resnet18_best.pth'
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        print(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        print("è¯·å…ˆè®­ç»ƒæ¨¡å‹")
        return
    
    # åˆ›å»ºé¢„æµ‹å™¨
    predictor = EmotionPredictor(model_path, config_path)
    
    # é¢„æµ‹ç¤ºä¾‹å›¾åƒ
    test_image_path = 'archive/test/happy/PrivateTest_10131363.jpg'
    
    if os.path.exists(test_image_path):
        result = predictor.predict(test_image_path)
        
        print("é¢„æµ‹ç»“æœ:")
        print(f"æƒ…ç»ª: {result['predicted_emotion']} {result['emoji']}")
        print(f"ç½®ä¿¡åº¦: {result['confidence']:.4f}")
        print("æ‰€æœ‰ç±»åˆ«æ¦‚ç‡:")
        for emotion, prob in result['all_probabilities'].items():
            print(f"  {emotion}: {prob:.4f}")
    else:
        print(f"æµ‹è¯•å›¾åƒä¸å­˜åœ¨: {test_image_path}")


def demo_webcam():
    """æ‘„åƒå¤´å®æ—¶é¢„æµ‹æ¼”ç¤º"""
    config_path = 'configs/config.yaml'
    model_path = 'models/saved_models/emotion_resnet_resnet18_best.pth'
    
    if not os.path.exists(model_path):
        print(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return
    
    # åˆ›å»ºå®æ—¶æ£€æµ‹å™¨
    detector = RealTimeEmotionDetector(model_path, config_path)
    
    # æ‰“å¼€æ‘„åƒå¤´
    cap = cv2.VideoCapture(0)
    
    print("æŒ‰ 'q' é”®é€€å‡ºæ‘„åƒå¤´é¢„è§ˆ")
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        # æ£€æµ‹å’Œé¢„æµ‹
        results = detector.detect_and_predict(frame)
        
        # ç»˜åˆ¶ç»“æœ
        frame = detector.draw_emotion_on_frame(frame, results)
        
        # æ˜¾ç¤ºç»“æœ
        cv2.imshow('æƒ…ç»ªè¯†åˆ«', frame)
        
        # é€€å‡ºæ¡ä»¶
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    
    cap.release()
    cv2.destroyAllWindows()


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='æƒ…ç»ªè¯†åˆ«æ¨ç†')
    parser.add_argument('--mode', choices=['image', 'webcam'], default='image',
                       help='è¿è¡Œæ¨¡å¼: image (å•å¼ å›¾åƒ) æˆ– webcam (æ‘„åƒå¤´)')
    
    args = parser.parse_args()
    
    if args.mode == 'image':
        demo_single_image()
    elif args.mode == 'webcam':
        demo_webcam()
