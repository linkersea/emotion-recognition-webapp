"""
æƒ…ç»ªè¯†åˆ«ResNetæ¨¡å‹æ¶æ„ - é›†æˆCBAMæ³¨æ„åŠ›æœºåˆ¶
"""

import torch
import torch.nn as nn
import torchvision.models as models


class ChannelAttention(nn.Module):
    """CBAMé€šé“æ³¨æ„åŠ›æ¨¡å—"""
    
    def __init__(self, in_channels, reduction=16):
        super(ChannelAttention, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)
        
        self.shared_mlp = nn.Sequential(
            nn.Conv2d(in_channels, in_channels // reduction, 1, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels // reduction, in_channels, 1, bias=False)
        )
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        avg_out = self.shared_mlp(self.avg_pool(x))
        max_out = self.shared_mlp(self.max_pool(x))
        return x * self.sigmoid(avg_out + max_out)


class SpatialAttention(nn.Module):
    """CBAMç©ºé—´æ³¨æ„åŠ›æ¨¡å—"""
    
    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()
        padding = 3 if kernel_size == 7 else 1
        self.conv = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        spatial_attention = self.conv(torch.cat([avg_out, max_out], dim=1))
        return x * self.sigmoid(spatial_attention)


class CBAM(nn.Module):
    """CBAM: Convolutional Block Attention Module"""
    
    def __init__(self, in_channels, reduction=16):
        super(CBAM, self).__init__()
        self.channel_attention = ChannelAttention(in_channels, reduction)
        self.spatial_attention = SpatialAttention()
    
    def forward(self, x):
        x = self.channel_attention(x)
        x = self.spatial_attention(x)
        return x


class EmotionResNet(nn.Module):
    """åŸºäºResNet+CBAMçš„æƒ…ç»ªè¯†åˆ«æ¨¡å‹"""
    
    def __init__(self, model_name='resnet18', num_classes=7, pretrained=True, dropout_rate=0.5, use_cbam=True):
        """
        Args:
            model_name: ResNetæ¨¡å‹åç§° ('resnet18', 'resnet34', 'resnet50')
            num_classes: æƒ…ç»ªç±»åˆ«æ•°é‡
            pretrained: æ˜¯å¦ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
            dropout_rate: Dropoutæ¯”ç‡
            use_cbam: æ˜¯å¦ä½¿ç”¨CBAMæ³¨æ„åŠ›æœºåˆ¶
        """
        super(EmotionResNet, self).__init__()
        
        self.model_name = model_name
        self.num_classes = num_classes
        self.use_cbam = use_cbam
        
        # åŠ è½½é¢„è®­ç»ƒçš„ResNetæ¨¡å‹
        if model_name == 'resnet18':
            if pretrained:
                self.backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)
            else:
                self.backbone = models.resnet18(weights=None)
            feature_dim = 512
        elif model_name == 'resnet34':
            if pretrained:
                self.backbone = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1)
            else:
                self.backbone = models.resnet34(weights=None)
            feature_dim = 512
        elif model_name == 'resnet50':
            if pretrained:
                self.backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)
            else:
                self.backbone = models.resnet50(weights=None)
            feature_dim = 2048
        else:
            raise ValueError(f"Unsupported model: {model_name}")
        
        # ç§»é™¤æœ€åçš„å¹³å‡æ± åŒ–å’Œå…¨è¿æ¥å±‚ï¼Œä¿ç•™ç‰¹å¾å›¾
        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])
        
        # æ·»åŠ CBAMæ³¨æ„åŠ›æœºåˆ¶
        if use_cbam:
            self.cbam = CBAM(feature_dim)
            print(f"âœ… CBAMæ³¨æ„åŠ›æœºåˆ¶å·²å¯ç”¨ (reduction=16)")
        else:
            self.cbam = None
            print("âŒ æœªä½¿ç”¨CBAMæ³¨æ„åŠ›æœºåˆ¶")
        
        # æ·»åŠ è‡ªå®šä¹‰åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Dropout(dropout_rate),
            nn.Linear(feature_dim, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate),
            nn.Linear(256, num_classes)
        )
        
        # åˆå§‹åŒ–æ–°å±‚çš„æƒé‡
        self._initialize_weights()
    
    def _initialize_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for m in self.classifier.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        # ç‰¹å¾æå–
        features = self.backbone(x)
        
        # åº”ç”¨CBAMæ³¨æ„åŠ›æœºåˆ¶
        if self.cbam is not None:
            features = self.cbam(features)
        
        # åˆ†ç±»
        output = self.classifier(features)
        
        return output
    
    def get_features(self, x):
        """è·å–ç‰¹å¾å‘é‡ï¼ˆç”¨äºå¯è§†åŒ–åˆ†æï¼‰"""
        with torch.no_grad():
            features = self.backbone(x)
            features = features.view(features.size(0), -1)
        return features


class EmotionResNetWithAttention(nn.Module):
    """å¸¦æ³¨æ„åŠ›æœºåˆ¶çš„ResNetæƒ…ç»ªè¯†åˆ«æ¨¡å‹"""
    
    def __init__(self, model_name='resnet18', num_classes=7, pretrained=True, dropout_rate=0.5):
        super(EmotionResNetWithAttention, self).__init__()
        
        # åŸºç¡€ResNetéª¨å¹²ç½‘ç»œ
        if model_name == 'resnet18':
            self.backbone = models.resnet18(pretrained=pretrained)
            feature_dim = 512
        elif model_name == 'resnet34':
            self.backbone = models.resnet34(pretrained=pretrained)
            feature_dim = 512
        elif model_name == 'resnet50':
            self.backbone = models.resnet50(pretrained=pretrained)
            feature_dim = 2048
        
        # ç§»é™¤æœ€åçš„å¹³å‡æ± åŒ–å’Œå…¨è¿æ¥å±‚
        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])
        
        # æ³¨æ„åŠ›æœºåˆ¶
        self.attention = nn.Sequential(
            nn.Conv2d(feature_dim, feature_dim // 8, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(feature_dim // 8, 1, 1),
            nn.Sigmoid()
        )
        
        # åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Dropout(dropout_rate),
            nn.Linear(feature_dim, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate),
            nn.Linear(256, num_classes)
        )
    
    def forward(self, x):
        # ç‰¹å¾æå–
        features = self.backbone(x)
        
        # æ³¨æ„åŠ›æƒé‡
        attention_weights = self.attention(features)
        
        # åº”ç”¨æ³¨æ„åŠ›
        attended_features = features * attention_weights
        
        # åˆ†ç±»
        output = self.classifier(attended_features)
        
        return output


def create_model(config):
    """æ ¹æ®é…ç½®åˆ›å»ºæ¨¡å‹"""
    architecture = config['model'].get('architecture', 'resnet').lower()
    
    if architecture == 'resnet':
        model = EmotionResNet(
            model_name=config['model']['name'].lower(),
            num_classes=config['model']['num_classes'],
            pretrained=config['model']['pretrained'],
            dropout_rate=config['model']['dropout_rate'],
            use_cbam=config['model'].get('use_cbam', True)
        )
        print(f"ğŸ—ï¸ ä½¿ç”¨ResNetæ¶æ„: {config['model']['name']}")
        
    elif architecture == 'vgg':
        # å¯¼å…¥VGGæ¨¡å‹
        try:
            from models.architectures.vgg_model import create_vgg_model
            model = create_vgg_model(config)
            print(f"ğŸ—ï¸ ä½¿ç”¨VGGæ¶æ„: {config['model']['name']}")
        except ImportError:
            print("âŒ VGGæ¨¡å‹æœªæ‰¾åˆ°ï¼Œå›é€€åˆ°ResNet")
            model = EmotionResNet(
                model_name='resnet18',
                num_classes=config['model']['num_classes'],
                pretrained=config['model']['pretrained'],
                dropout_rate=config['model']['dropout_rate'],
                use_cbam=False
            )
            
    elif architecture == 'custom_cnn':
        try:
            from models.architectures.vgg_model import create_custom_cnn
            model = create_custom_cnn(config)
            print("ğŸ—ï¸ ä½¿ç”¨è‡ªå®šä¹‰CNNæ¶æ„")
        except ImportError:
            print("âŒ è‡ªå®šä¹‰CNNæœªæ‰¾åˆ°ï¼Œå›é€€åˆ°ResNet")
            model = EmotionResNet(
                model_name='resnet18',
                num_classes=config['model']['num_classes'],
                pretrained=config['model']['pretrained'],
                dropout_rate=config['model']['dropout_rate'],
                use_cbam=False
            )
    else:
        print(f"âŒ æœªçŸ¥æ¶æ„ {architecture}ï¼Œä½¿ç”¨é»˜è®¤ResNet")
        model = EmotionResNet(
            model_name='resnet18',
            num_classes=config['model']['num_classes'],
            pretrained=config['model']['pretrained'],
            dropout_rate=config['model']['dropout_rate'],
            use_cbam=True
        )
    
    return model


def count_parameters(model):
    """ç»Ÿè®¡æ¨¡å‹å‚æ•°æ•°é‡"""
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"æ€»å‚æ•°æ•°: {total_params:,}")
    print(f"å¯è®­ç»ƒå‚æ•°æ•°: {trainable_params:,}")
    
    return total_params, trainable_params


if __name__ == "__main__":
    import yaml
    
    # åŠ è½½é…ç½®
    with open('../../configs/config.yaml', 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # åˆ›å»ºæ¨¡å‹
    model = create_model(config)
    
    # ç»Ÿè®¡å‚æ•°
    count_parameters(model)
    
    # æµ‹è¯•æ¨¡å‹
    dummy_input = torch.randn(1, 3, 48, 48)
    output = model(dummy_input)
    print(f"è¾“å…¥å½¢çŠ¶: {dummy_input.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ: {torch.softmax(output, dim=1)}")
